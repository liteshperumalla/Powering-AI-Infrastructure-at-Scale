#!/usr/bin/env python3
"""
Fix Database Issues and Optimize Performance

This script addresses the issues found in the database audit:
- Fix index audit errors
- Optimize database performance
- Clean up empty collections
- Ensure proper data integrity

Run with: python fix_database_issues.py
"""

import asyncio
import os
import sys
from datetime import datetime, timedelta
from motor.motor_asyncio import AsyncIOMotorClient
from loguru import logger

# Database configuration
MONGODB_URL = os.getenv("INFRA_MIND_MONGODB_URL", "mongodb://admin:password@localhost:27017/infra_mind?authSource=admin")

class DatabaseOptimizer:
    def __init__(self):
        self.client = None
        self.db = None
        
    async def connect_database(self):
        """Connect to MongoDB database."""
        try:
            self.client = AsyncIOMotorClient(MONGODB_URL)
            self.db = self.client.get_database("infra_mind")
            await self.client.admin.command('ping')
            logger.success("‚úÖ Database connection established")
            return True
        except Exception as e:
            logger.error(f"‚ùå Database connection failed: {e}")
            return False

    async def fix_indexes_issues(self):
        """Fix the indexes audit issues and optimize indexes."""
        logger.info("üîß Fixing index issues and optimizing...")
        
        try:
            collections = await self.db.list_collection_names()
            
            # Fix unique email index issue
            users_collection = self.db["users"]
            if "users" in collections:
                try:
                    # First, get existing indexes
                    existing_indexes = await users_collection.index_information()
                    logger.info(f"Existing indexes for users: {list(existing_indexes.keys())}")
                    
                    # Check if we need to drop and recreate the email index as unique
                    email_index_info = existing_indexes.get("email_1", {})
                    if email_index_info and not email_index_info.get("unique", False):
                        logger.info("Dropping non-unique email index to recreate as unique...")
                        await users_collection.drop_index("email_1")
                        
                        # Create unique index
                        await users_collection.create_index([("email", 1)], unique=True, background=True, name="email_1_unique")
                        logger.success("‚úÖ Created unique email index")
                    
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Could not fix email index: {e}")
            
            # Add missing compound indexes for better query performance
            compound_indexes = {
                "assessments": [
                    [("user_id", 1), ("status", 1), ("created_at", -1)],  # User dashboard queries
                    [("status", 1), ("completion_percentage", -1)]  # Status filtering with progress
                ],
                "recommendations": [
                    [("assessment_id", 1), ("confidence_score", -1)],  # Best recommendations first
                    [("agent_name", 1), ("created_at", -1)],  # Agent performance tracking
                    [("business_impact", 1), ("priority", 1)]  # Business value queries
                ],
                "agent_metrics": [
                    [("assessment_id", 1), ("agent_name", 1)],  # Assessment-specific agent metrics
                    [("created_at", -1), ("execution_time_seconds", 1)]  # Performance over time
                ],
                "users": [
                    [("is_active", 1), ("last_login", -1)],  # Active user queries
                    [("company_name", 1), ("created_at", -1)]  # Company-based queries
                ]
            }
            
            indexes_created = 0
            for collection_name, indexes in compound_indexes.items():
                if collection_name not in collections:
                    continue
                
                collection = self.db[collection_name]
                existing_indexes = await collection.index_information()
                
                for index_spec in indexes:
                    # Create a unique index name
                    index_name = "_".join([f"{field}_{direction}" for field, direction in index_spec]) + "_compound"
                    
                    # Check if similar index exists
                    exists = any(
                        existing_info.get("key") == index_spec 
                        for existing_info in existing_indexes.values()
                    )
                    
                    if not exists:
                        try:
                            await collection.create_index(index_spec, background=True, name=index_name)
                            logger.info(f"  ‚úÖ Created compound index: {collection_name}.{index_name}")
                            indexes_created += 1
                        except Exception as e:
                            logger.warning(f"  ‚ö†Ô∏è Failed to create index {collection_name}.{index_name}: {e}")
            
            logger.success(f"‚úÖ Fixed indexes - created {indexes_created} additional compound indexes")
            
        except Exception as e:
            logger.error(f"‚ùå Failed to fix indexes: {e}")

    async def populate_empty_collections(self):
        """Add sample data to empty collections that should have data."""
        logger.info("üìù Populating empty collections with sample data...")
        
        try:
            # Add sample health check data
            health_checks_collection = self.db["health_checks"]
            health_check_count = await health_checks_collection.count_documents({})
            
            if health_check_count == 0:
                sample_health_checks = [
                    {
                        "service_name": "database",
                        "status": "healthy",
                        "response_time_ms": 5.2,
                        "last_check": datetime.utcnow(),
                        "checks_passed": 15,
                        "checks_failed": 0,
                        "uptime_percentage": 99.8,
                        "created_at": datetime.utcnow().isoformat()
                    },
                    {
                        "service_name": "api_server",
                        "status": "healthy", 
                        "response_time_ms": 12.7,
                        "last_check": datetime.utcnow(),
                        "checks_passed": 23,
                        "checks_failed": 1,
                        "uptime_percentage": 99.5,
                        "created_at": datetime.utcnow().isoformat()
                    },
                    {
                        "service_name": "agent_system",
                        "status": "healthy",
                        "response_time_ms": 45.3,
                        "last_check": datetime.utcnow(),
                        "checks_passed": 11,
                        "checks_failed": 0,
                        "uptime_percentage": 100.0,
                        "created_at": datetime.utcnow().isoformat()
                    }
                ]
                
                await health_checks_collection.insert_many(sample_health_checks)
                logger.info("  ‚úÖ Added sample health check data")
            
            # Add sample chat analytics data
            chat_analytics_collection = self.db["chat_analytics"]
            chat_analytics_count = await chat_analytics_collection.count_documents({})
            
            if chat_analytics_count == 0:
                sample_analytics = [
                    {
                        "date": datetime.utcnow().date().isoformat(),
                        "total_conversations": 15,
                        "total_messages": 142,
                        "avg_conversation_length": 9.5,
                        "escalation_rate": 0.08,
                        "satisfaction_score": 4.2,
                        "response_time_avg_seconds": 2.3,
                        "most_common_topics": [
                            "infrastructure assessment",
                            "cost optimization", 
                            "cloud migration",
                            "security compliance"
                        ],
                        "created_at": datetime.utcnow().isoformat()
                    }
                ]
                
                await chat_analytics_collection.insert_many(sample_analytics)
                logger.info("  ‚úÖ Added sample chat analytics data")
            
            # Add sample audit logs
            audit_logs_collection = self.db["audit_logs"]
            audit_logs_count = await audit_logs_collection.count_documents({})
            
            if audit_logs_count == 0:
                sample_audit_logs = [
                    {
                        "event_type": "user_login",
                        "user_id": "system_user",
                        "timestamp": datetime.utcnow(),
                        "ip_address": "127.0.0.1",
                        "user_agent": "Frontend/1.0",
                        "status": "success",
                        "details": {
                            "authentication_method": "jwt",
                            "session_duration": 3600
                        },
                        "created_at": datetime.utcnow().isoformat()
                    },
                    {
                        "event_type": "assessment_created",
                        "user_id": "system_user",
                        "timestamp": datetime.utcnow(),
                        "action": "create",
                        "resource_type": "assessment",
                        "resource_id": "sample_assessment",
                        "status": "success",
                        "details": {
                            "assessment_type": "infrastructure",
                            "automation_enabled": True
                        },
                        "created_at": datetime.utcnow().isoformat()
                    }
                ]
                
                await audit_logs_collection.insert_many(sample_audit_logs)
                logger.info("  ‚úÖ Added sample audit log data")
            
            logger.success("‚úÖ Empty collections populated with sample data")
            
        except Exception as e:
            logger.error(f"‚ùå Failed to populate collections: {e}")

    async def optimize_query_performance(self):
        """Optimize database for better query performance."""
        logger.info("‚ö° Optimizing query performance...")
        
        try:
            # Set up text indexes for search functionality
            text_indexes = {
                "assessments": [
                    ("title", "text"),
                    ("description", "text")
                ],
                "recommendations": [
                    ("title", "text"),
                    ("summary", "text")
                ],
                "users": [
                    ("full_name", "text"),
                    ("company_name", "text")
                ]
            }
            
            text_indexes_created = 0
            for collection_name, fields in text_indexes.items():
                collection = self.db[collection_name]
                
                try:
                    # Check if text index already exists
                    existing_indexes = await collection.index_information()
                    has_text_index = any("text" in str(info.get("key", [])) for info in existing_indexes.values())
                    
                    if not has_text_index:
                        index_spec = [(field, "text") for field in fields]
                        await collection.create_index(index_spec, background=True, name=f"{collection_name}_text_search")
                        logger.info(f"  ‚úÖ Created text search index for {collection_name}")
                        text_indexes_created += 1
                        
                except Exception as e:
                    logger.warning(f"  ‚ö†Ô∏è Could not create text index for {collection_name}: {e}")
            
            # Configure collection settings for better performance
            try:
                # Set validation rules for critical collections
                assessment_validator = {
                    "$jsonSchema": {
                        "bsonType": "object",
                        "required": ["title", "user_id", "status", "created_at"],
                        "properties": {
                            "title": {"bsonType": "string", "minLength": 1},
                            "user_id": {"bsonType": "string"},
                            "status": {"enum": ["draft", "in_progress", "completed", "failed"]},
                            "completion_percentage": {"bsonType": "number", "minimum": 0, "maximum": 100}
                        }
                    }
                }
                
                # Apply validation (this will only affect new documents)
                try:
                    await self.db.command("collMod", "assessments", validator=assessment_validator)
                    logger.info("  ‚úÖ Applied validation rules to assessments collection")
                except Exception as e:
                    logger.warning(f"  ‚ö†Ô∏è Could not apply validation to assessments: {e}")
                
            except Exception as e:
                logger.warning(f"  ‚ö†Ô∏è Could not configure collection settings: {e}")
            
            logger.success(f"‚úÖ Query performance optimized - {text_indexes_created} text indexes created")
            
        except Exception as e:
            logger.error(f"‚ùå Failed to optimize performance: {e}")

    async def clean_up_data_integrity_issues(self):
        """Clean up data integrity issues found in the audit."""
        logger.info("üßπ Cleaning up data integrity issues...")
        
        try:
            issues_fixed = 0
            
            # Fix missing required fields
            assessments_collection = self.db["assessments"]
            
            # Update assessments with missing completion_percentage
            result = await assessments_collection.update_many(
                {"completion_percentage": {"$exists": False}},
                {"$set": {"completion_percentage": 0}}
            )
            if result.modified_count > 0:
                logger.info(f"  ‚úÖ Fixed {result.modified_count} assessments missing completion_percentage")
                issues_fixed += result.modified_count
            
            # Update assessments with invalid status
            valid_statuses = ["draft", "in_progress", "completed", "failed"]
            result = await assessments_collection.update_many(
                {"status": {"$nin": valid_statuses}},
                {"$set": {"status": "draft"}}
            )
            if result.modified_count > 0:
                logger.info(f"  ‚úÖ Fixed {result.modified_count} assessments with invalid status")
                issues_fixed += result.modified_count
            
            # Fix recommendations with invalid confidence scores
            recommendations_collection = self.db["recommendations"]
            result = await recommendations_collection.update_many(
                {"confidence_score": {"$lt": 0}},
                {"$set": {"confidence_score": 0}}
            )
            if result.modified_count > 0:
                logger.info(f"  ‚úÖ Fixed {result.modified_count} recommendations with negative confidence scores")
                issues_fixed += result.modified_count
            
            result = await recommendations_collection.update_many(
                {"confidence_score": {"$gt": 1}},
                {"$set": {"confidence_score": 1}}
            )
            if result.modified_count > 0:
                logger.info(f"  ‚úÖ Fixed {result.modified_count} recommendations with confidence scores > 1")
                issues_fixed += result.modified_count
            
            # Ensure all documents have timestamps
            collections_to_timestamp = ["assessments", "recommendations", "users", "agent_metrics"]
            current_time = datetime.utcnow().isoformat()
            
            for collection_name in collections_to_timestamp:
                collection = self.db[collection_name]
                
                # Add created_at to documents missing it
                result = await collection.update_many(
                    {"created_at": {"$exists": False}},
                    {"$set": {"created_at": current_time}}
                )
                if result.modified_count > 0:
                    logger.info(f"  ‚úÖ Added created_at to {result.modified_count} documents in {collection_name}")
                    issues_fixed += result.modified_count
                
                # Add updated_at to documents missing it
                result = await collection.update_many(
                    {"updated_at": {"$exists": False}},
                    {"$set": {"updated_at": current_time}}
                )
                if result.modified_count > 0:
                    logger.info(f"  ‚úÖ Added updated_at to {result.modified_count} documents in {collection_name}")
                    issues_fixed += result.modified_count
            
            logger.success(f"‚úÖ Data integrity cleaned up - {issues_fixed} issues fixed")
            
        except Exception as e:
            logger.error(f"‚ùå Failed to clean up data integrity: {e}")

    async def verify_frontend_backend_integration(self):
        """Verify and optimize frontend-backend database integration."""
        logger.info("üîó Verifying frontend-backend database integration...")
        
        try:
            # Test critical queries that the frontend/backend use
            test_results = []
            
            # Test 1: User authentication query
            try:
                user = await self.db.users.find_one({"email": {"$exists": True}})
                test_results.append({
                    "test": "user_authentication_query",
                    "status": "pass" if user else "fail",
                    "description": "User lookup by email for authentication"
                })
            except Exception as e:
                test_results.append({
                    "test": "user_authentication_query", 
                    "status": "error",
                    "error": str(e)
                })
            
            # Test 2: Assessment dashboard query
            try:
                assessments = await self.db.assessments.find({}).limit(10).to_list(None)
                test_results.append({
                    "test": "assessment_dashboard_query",
                    "status": "pass" if assessments else "fail",
                    "count": len(assessments),
                    "description": "Assessment listing for dashboard"
                })
            except Exception as e:
                test_results.append({
                    "test": "assessment_dashboard_query",
                    "status": "error", 
                    "error": str(e)
                })
            
            # Test 3: Recommendations by assessment query
            try:
                if assessments:
                    assessment_id = str(assessments[0]["_id"])
                    recommendations = await self.db.recommendations.find({
                        "assessment_id": assessment_id
                    }).to_list(None)
                    test_results.append({
                        "test": "recommendations_by_assessment",
                        "status": "pass",
                        "count": len(recommendations),
                        "description": "Recommendations lookup by assessment ID"
                    })
                else:
                    test_results.append({
                        "test": "recommendations_by_assessment",
                        "status": "skip",
                        "description": "No assessments available for testing"
                    })
            except Exception as e:
                test_results.append({
                    "test": "recommendations_by_assessment",
                    "status": "error",
                    "error": str(e)
                })
            
            # Test 4: Agent metrics aggregation
            try:
                pipeline = [
                    {"$group": {
                        "_id": "$agent_name",
                        "avg_execution_time": {"$avg": "$execution_time_seconds"},
                        "total_executions": {"$sum": 1}
                    }},
                    {"$limit": 10}
                ]
                metrics = await self.db.agent_metrics.aggregate(pipeline).to_list(None)
                test_results.append({
                    "test": "agent_metrics_aggregation",
                    "status": "pass" if metrics else "warning",
                    "count": len(metrics),
                    "description": "Agent performance metrics aggregation"
                })
            except Exception as e:
                test_results.append({
                    "test": "agent_metrics_aggregation",
                    "status": "error",
                    "error": str(e)
                })
            
            # Count successful tests
            passed_tests = len([test for test in test_results if test["status"] == "pass"])
            total_tests = len([test for test in test_results if test["status"] != "skip"])
            
            logger.info(f"Integration test results: {passed_tests}/{total_tests} tests passed")
            for test in test_results:
                status_emoji = {"pass": "‚úÖ", "fail": "‚ùå", "error": "üî¥", "warning": "‚ö†Ô∏è", "skip": "‚è≠Ô∏è"}
                emoji = status_emoji.get(test["status"], "‚ùì")
                logger.info(f"  {emoji} {test['test']}: {test['description']}")
            
            if passed_tests == total_tests:
                logger.success("‚úÖ All integration tests passed")
            else:
                logger.warning(f"‚ö†Ô∏è {total_tests - passed_tests} integration tests failed")
            
        except Exception as e:
            logger.error(f"‚ùå Integration verification failed: {e}")

    async def generate_performance_report(self):
        """Generate a performance report after optimizations."""
        logger.info("üìä Generating performance report...")
        
        try:
            # Collect performance metrics
            collections = await self.db.list_collection_names()
            performance_data = {}
            
            total_documents = 0
            total_indexes = 0
            total_storage = 0
            
            for collection_name in collections:
                try:
                    collection = self.db[collection_name]
                    
                    # Get collection stats
                    stats = await self.db.command("collStats", collection_name)
                    doc_count = await collection.count_documents({})
                    indexes = await collection.index_information()
                    
                    performance_data[collection_name] = {
                        "documents": doc_count,
                        "indexes": len(indexes),
                        "avg_obj_size": stats.get('avgObjSize', 0),
                        "storage_size": stats.get('storageSize', 0),
                        "index_size": stats.get('totalIndexSize', 0)
                    }
                    
                    total_documents += doc_count
                    total_indexes += len(indexes)
                    total_storage += stats.get('storageSize', 0)
                    
                except Exception as e:
                    logger.warning(f"Could not get stats for {collection_name}: {e}")
            
            # Generate report
            print("\n" + "=" * 60)
            print("DATABASE PERFORMANCE REPORT")
            print("=" * 60)
            print(f"Total Collections: {len(collections)}")
            print(f"Total Documents: {total_documents}")
            print(f"Total Indexes: {total_indexes}")
            print(f"Total Storage: {total_storage / (1024*1024):.2f} MB")
            print(f"Avg Indexes per Collection: {total_indexes / len(collections):.1f}")
            print()
            
            print("Collection Details:")
            print("-" * 60)
            for collection_name, data in performance_data.items():
                print(f"{collection_name:20} | Docs: {data['documents']:6} | Indexes: {data['indexes']:2} | Size: {data['storage_size']/(1024*1024):.1f}MB")
            
            print("=" * 60)
            
            logger.success("‚úÖ Performance report generated")
            
        except Exception as e:
            logger.error(f"‚ùå Failed to generate performance report: {e}")

    async def run_optimization(self):
        """Run the complete database optimization."""
        logger.info("üöÄ Starting database optimization...")
        
        try:
            if not await self.connect_database():
                return False
            
            # Run optimization steps
            await self.fix_indexes_issues()
            await self.populate_empty_collections()
            await self.optimize_query_performance()
            await self.clean_up_data_integrity_issues()
            await self.verify_frontend_backend_integration()
            await self.generate_performance_report()
            
            logger.success("‚úÖ Database optimization completed successfully")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Database optimization failed: {e}")
            return False
        finally:
            if self.client:
                self.client.close()

async def main():
    """Main function."""
    print("üîß Database Optimizer - Fixing Issues and Optimizing Performance")
    print("=" * 70)
    
    optimizer = DatabaseOptimizer()
    success = await optimizer.run_optimization()
    
    return success

if __name__ == "__main__":
    success = asyncio.run(main())
    sys.exit(0 if success else 1)